
## 2019/7/30

- Try different Loss function: L1, MSE
- validate during training process
- Position-wise feedforward layer
- Implement multi-head attention layer

Possible ideas:
1. LSTM -> Attention -> GRU/LSTM -> Attention -> Feedforward NN

## 2018/7/31

- Try hierarchical attention
- Try transformer encoder
