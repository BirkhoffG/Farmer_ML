{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creates all form combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unusedYears = ['2000','2001','2002','2003','2004','2005','2006','2007']\n",
    "year = ['2008','2009','2010','2011','2012','2013','2014','2015','2016','2017','2018']\n",
    "month = ['January','February','March','April','May','June','July',\n",
    "         'August','September','October','November','December']\n",
    "states = ['Andaman and Nicobar','Andhra Pradesh','Arunachal Pradesh','Assam','Bihar','Chandigarh',\n",
    "          'Chattisgarh','Dadra and Nagar Haveli','Daman and Diu','Goa','Gujarat','Haryana','Himachal Pradesh',\n",
    "          'Jammu and Kashmir','Jharkhand','Karnataka','Kerala','Lakshadweep','Madhya Pradesh','Maharashtra',\n",
    "          'Manipur','Meghalaya','Mizoram','Nagaland','NCT of Delhi','Odisha','Pondicherry','Punjab','Rajasthan',\n",
    "          'Tamil Nadu','Telangana','Tripura','Uttar Pradesh','Uttrakhand','West Bengal']\n",
    "crops = ['Brinjal','Tomato','Mango','Cauliflower','Pointed gourd (Parval)','Green Chilli']\n",
    "datesAndCrops = []\n",
    "\n",
    "for l in range(len(crops)):\n",
    "    for i in range(len(year)):\n",
    "        for j in range(len(month)):\n",
    "            for k in range(len(states)):\n",
    "                datesAndCrops.append(year[i]+' '+month[j]+' '+states[k]+' '+crops[l])\n",
    "                #saveFile = month[j][:3]+year[i][2:]+states[k][:3]+crops[l][:4]+'.csv'\n",
    "                #print(saveFile)\n",
    "        \n",
    "len(datesAndCrops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual query scrape automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.support.expected_conditions.presence_of_element_located object at 0x1108a0a58>\n",
      "No Table\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "def checkTableExists(table):\n",
    "    try:\n",
    "        driver.find_element_by_id(\"cphBody_gridRecords\")\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "url = 'http://agmarknet.gov.in/PriceAndArrivals/DatewiseCommodityReport.aspx'\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "year = driver.find_element_by_id(\"cphBody_cboYear\")\n",
    "year.send_keys(\"2008\")\n",
    "\n",
    "month = driver.find_element_by_id(\"cphBody_cboMonth\")\n",
    "month.send_keys(\"January\")\n",
    "\n",
    "#wait stage\n",
    "wait = WebDriverWait(driver, 60)\n",
    "wait.until(EC.presence_of_element_located((By.ID, 'cphBody_cboState')))\n",
    "\n",
    "state = driver.find_element_by_id(\"cphBody_cboState\")\n",
    "state.send_keys(\"Bihar\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "wait.until(EC.presence_of_element_located((By.ID, 'cphBody_cboCommodity')))\n",
    "\n",
    "crop = driver.find_element_by_id(\"cphBody_cboCommodity\")\n",
    "crop.send_keys(\"Cauliflower\")\n",
    "\n",
    "wait = WebDriverWait(driver, 60)\n",
    "wait.until(EC.presence_of_element_located((By.ID, 'cphBody_btnSubmit')))\n",
    "print(EC.presence_of_element_located((By.ID, 'cphBody_btnSubmit')))\n",
    "\n",
    "driver.find_element_by_name(\"ctl00$cphBody$btnSubmit\").click()\n",
    "#table\n",
    "time.sleep(5)\n",
    "\n",
    "#wait = WebDriverWait(driver, 3)\n",
    "#elem = wait.until(EC.visibility_of_any_elements_located((By.ID, \"cphBody_gridRecords\")))\n",
    "\n",
    "\n",
    "if(checkTableExists('cphBody_gridRecords')):\n",
    "\n",
    "    tab_data = driver.find_element_by_id(\"cphBody_gridRecords\")\n",
    "\n",
    "    list_rows = [[cell.text for cell in row.find_elements_by_tag_name('td')]\n",
    "                 for row in tab_data.find_elements_by_tag_name('tr')]\n",
    "    #print(list_rows)\n",
    "    df = pd.DataFrame(list_rows)\n",
    "    df.columns=['Market', 'Arrival Date','Arrivals (Tonnes)','Variety','Minimum Price(Rs./Quintal)',\n",
    "                'Maximum Price(Rs./Quintal)','Modal Price(Rs./Quintal)']\n",
    "    df = df.iloc[1:]\n",
    "    df = df.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "    for val in range(len(df['Market'])):\n",
    "        if(pd.isnull(df['Market'].values[val])):\n",
    "            #Sets nan value to current market\n",
    "            df['Market'].values[val] = market\n",
    "        else:\n",
    "            #Sets the market to the current value, indicates a new market\n",
    "            market=df['Market'].values[val]\n",
    "\n",
    "    df.to_csv(\"Final Data/testFile.csv\",index=False)\n",
    "    print('File testFile completed and saved')\n",
    "\n",
    "else:\n",
    "    print('No Table')\n",
    "    driver.back()\n",
    "        \n",
    "#print(df.at[1,6])\n",
    "#should print 1250\n",
    "#driver.back()\n",
    "\n",
    "#scrape\n",
    "#click back button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Cumulative query scrape automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: 2013 Green Chilli\n",
      "No state for: Jan13AndaGree.csv. No file saved\n",
      "File Jan13AndhGree.csv scraped and saved\n",
      "No state for: Jan13ArunGree.csv. No file saved\n",
      "File Jan13AssaGree.csv scraped and saved\n",
      "No table for: Jan13BihaGree.csv. No file saved\n",
      "File Jan13ChanGree.csv scraped and saved\n",
      "No table for: Jan13ChatGree.csv. No file saved\n",
      "No state for: Jan13DadrGree.csv. No file saved\n",
      "No state for: Jan13DamaGree.csv. No file saved\n",
      "File Jan13GoaGree.csv scraped and saved\n",
      "File Jan13GujaGree.csv scraped and saved\n",
      "File Jan13HaryGree.csv scraped and saved\n",
      "File Jan13HimaGree.csv scraped and saved\n",
      "File Jan13JammGree.csv scraped and saved\n",
      "File Jan13JharGree.csv scraped and saved\n",
      "File Jan13KarnGree.csv scraped and saved\n",
      "File Jan13KeraGree.csv scraped and saved\n",
      "No state for: Jan13LaksGree.csv. No file saved\n",
      "File Jan13MadhGree.csv scraped and saved\n",
      "File Jan13MahaGree.csv scraped and saved\n",
      "No state for: Jan13ManiGree.csv. No file saved\n",
      "File Jan13MeghGree.csv scraped and saved\n",
      "No state for: Jan13MizoGree.csv. No file saved\n",
      "No state for: Jan13NagaGree.csv. No file saved\n",
      "File Jan13NCT Gree.csv scraped and saved\n",
      "File Jan13OdisGree.csv scraped and saved\n",
      "No table for: Jan13PondGree.csv. No file saved\n",
      "File Jan13PunjGree.csv scraped and saved\n",
      "File Jan13RajaGree.csv scraped and saved\n",
      "No table for: Jan13TamiGree.csv. No file saved\n",
      "File Jan13TelaGree.csv scraped and saved\n",
      "No table for: Jan13TripGree.csv. No file saved\n",
      "File Jan13UttaGree.csv scraped and saved\n",
      "File Jan13UttrGree.csv scraped and saved\n",
      "File Jan13WestGree.csv scraped and saved\n",
      "It took 35.34560037851334 minutes to execute January 2013 for all states. Start: 11:41:27 End: 12:16:47\n",
      "Around 12:52:08 is when the next month is expected to finish\n",
      "\n",
      "Created log file\n",
      "Logs saved\n",
      "No state for: Feb13AndaGree.csv. No file saved\n",
      "File Feb13AndhGree.csv scraped and saved\n",
      "No state for: Feb13ArunGree.csv. No file saved\n",
      "File Feb13AssaGree.csv scraped and saved\n",
      "No table for: Feb13BihaGree.csv. No file saved\n",
      "File Feb13ChanGree.csv scraped and saved\n",
      "No table for: Feb13ChatGree.csv. No file saved\n",
      "No state for: Feb13DadrGree.csv. No file saved\n",
      "No state for: Feb13DamaGree.csv. No file saved\n",
      "File Feb13GoaGree.csv scraped and saved\n",
      "File Feb13GujaGree.csv scraped and saved\n",
      "File Feb13HaryGree.csv scraped and saved\n",
      "File Feb13HimaGree.csv scraped and saved\n",
      "File Feb13JammGree.csv scraped and saved\n",
      "File Feb13JharGree.csv scraped and saved\n",
      "File Feb13KarnGree.csv scraped and saved\n",
      "File Feb13KeraGree.csv scraped and saved\n",
      "No state for: Feb13LaksGree.csv. No file saved\n",
      "File Feb13MadhGree.csv scraped and saved\n",
      "File Feb13MahaGree.csv scraped and saved\n",
      "No state for: Feb13ManiGree.csv. No file saved\n",
      "File Feb13MeghGree.csv scraped and saved\n",
      "File Feb13MizoGree.csv scraped and saved\n",
      "File Feb13NagaGree.csv scraped and saved\n",
      "File Feb13NCT Gree.csv scraped and saved\n",
      "File Feb13OdisGree.csv scraped and saved\n",
      "No table for: Feb13PondGree.csv. No file saved\n",
      "File Feb13PunjGree.csv scraped and saved\n",
      "File Feb13RajaGree.csv scraped and saved\n",
      "No table for: Feb13TamiGree.csv. No file saved\n",
      "File Feb13TelaGree.csv scraped and saved\n",
      "File Feb13TripGree.csv scraped and saved\n",
      "File Feb13UttaGree.csv scraped and saved\n",
      "File Feb13UttrGree.csv scraped and saved\n",
      "File Feb13WestGree.csv scraped and saved\n",
      "It took 59.12131628592809 minutes to execute February 2013 for all states. Start: 12:16:47 End: 13:15:55\n",
      "Around 14:03:09 is when the next month is expected to finish\n",
      "\n",
      "Appended to existing log file\n",
      "Logs saved\n",
      "No state for: Mar13AndaGree.csv. No file saved\n",
      "File Mar13AndhGree.csv scraped and saved\n",
      "No state for: Mar13ArunGree.csv. No file saved\n",
      "File Mar13AssaGree.csv scraped and saved\n",
      "No table for: Mar13BihaGree.csv. No file saved\n",
      "File Mar13ChanGree.csv scraped and saved\n",
      "No table for: Mar13ChatGree.csv. No file saved\n",
      "No state for: Mar13DadrGree.csv. No file saved\n",
      "No state for: Mar13DamaGree.csv. No file saved\n",
      "File Mar13GoaGree.csv scraped and saved\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "#checks to see if element exists and returns appropriate boolean\n",
    "def checkElementExistsByID(table):\n",
    "    try:\n",
    "        driver.find_element_by_id(\"cphBody_gridRecords\")\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "#checks to see if searching term is present in the dropdown values and returns proper boolean\n",
    "def checkElementInDropdown(param, val):\n",
    "    inputParam = driver.find_element_by_id(param)\n",
    "    if(val in inputParam.text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#returns the average of all month timestamps for predicting runtime of next loop    \n",
    "def avg(runTimes):\n",
    "    if(len(runTimes)==0):\n",
    "        average = 0\n",
    "    else:\n",
    "        average = sum(runTimes)/len(runTimes)\n",
    "    return average\n",
    "\n",
    "#writes the logs to a text file for each year and crop\n",
    "def log(logs, logFileSave):\n",
    "    #sets folder for log storage\n",
    "    dirForLogs = 'Logs'\n",
    "    #creates the folder at the correct path\n",
    "    createFolder('./'+dirForLogs)\n",
    "    #navigates to new directory\n",
    "    standardPath = os.getcwd()+'/'+dirForLogs\n",
    "    #path plus the name of file to be saved\n",
    "    filename = standardPath+'/'+logFileSave\n",
    "\n",
    "    #check if the file exists\n",
    "    if os.path.exists(filename):\n",
    "        appendWrite = 'a' # append if already exists\n",
    "        print('Appended to existing log file')\n",
    "    else:\n",
    "        appendWrite = 'w' # make a new file if not\n",
    "        print('Created log file')\n",
    "\n",
    "    #open and write to file\n",
    "    file = open(filename, mode= appendWrite)\n",
    "    #file.write(str(logs))\n",
    "    for line in logs:\n",
    "        file.write(line+'\\n')\n",
    "    file.write('\\n')\n",
    "    #close the file to prevent errors\n",
    "    file.close()\n",
    "    \n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "            print('Created directory: '+directory[2:-1])\n",
    "    except OSError:\n",
    "        print ('Error: Creating directory. ' +  directory)\n",
    "\n",
    "\n",
    "#lists of scraped and unscraped pages after algorithm is run\n",
    "scrapedPages = []\n",
    "unscrapedPages = []\n",
    "\n",
    "#sets parameters\n",
    "unusedYears = ['2000','2001','2002','2003','2004','2005','2006','2007', '2008','2009','2010',\n",
    "               '2011','2012','2013','2014','2016','2017','2018']\n",
    "years = ['2013']\n",
    "months = ['January','February','March','April']#,'May','June','July',\n",
    "         #'August','September','October','November','December']\n",
    "states = ['Andamans and Nicobar','Andhra Pradesh','Arunachal Pradesh','Assam','Bihar','Chandigarh',\n",
    "          'Chattisgarh','Dadra and Nagar Haveli','Daman and Diu','Goa','Gujarat','Haryana','Himachal Pradesh',\n",
    "          'Jammu and Kashmir','Jharkhand','Karnataka','Kerala','Lakshadweep','Madhya Pradesh','Maharashtra',\n",
    "          'Manipur','Meghalaya','Mizoram','Nagaland','NCT of Delhi','Odisha','Pondicherry','Punjab','Rajasthan',\n",
    "          'Tamil Nadu','Telangana','Tripura','Uttar Pradesh','Uttrakhand','West Bengal']\n",
    "crops = ['Green Chilli']\n",
    "\n",
    "#list of all pages queried\n",
    "datesAndCrops = []\n",
    "runTimes = []\n",
    "\n",
    "#file names logs will be stored under\n",
    "logFileSave = []\n",
    "#logs from script\n",
    "logs = []\n",
    "\n",
    "url = 'http://agmarknet.gov.in/PriceAndArrivals/DatewiseCommodityReport.aspx'\n",
    "\n",
    "#creates a webdriver using chrome and navigates to URL\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "#loops through all possible combinations of form inputs given parameters above\n",
    "for l in range(len(crops)):\n",
    "    for i in range(len(years)):\n",
    "        #creates year crop folder for data to be stored\n",
    "        createFolder('./'+years[i]+' '+crops[l]+'/')\n",
    "        \n",
    "        #creates the save file for logs\n",
    "        logFileSave.append(years[i]+' '+crops[l]+' '+'Logs.txt')\n",
    "        \n",
    "        #start duration for month \n",
    "        start = time.time()\n",
    "        \n",
    "        #set initial time stamp\n",
    "        startTS = datetime.datetime.now()\n",
    "        \n",
    "        for j in range(len(months)):\n",
    "            for k in range(len(states)):\n",
    "\n",
    "                datesAndCrops.append(years[i]+' '+months[j]+' '+states[k]+' '+crops[l])\n",
    "                \n",
    "                #creates file names for each queried page regardless if it is used\n",
    "                saveFile = months[j][:3]+years[i][2:]+states[k][:4]+crops[l][:4]+'.csv'\n",
    "                \n",
    "                #inputs the year into the form\n",
    "                inputYear = driver.find_element_by_id(\"cphBody_cboYear\")\n",
    "                inputYear.send_keys(str(years[i]))\n",
    "                \n",
    "                #inputs the month into the form\n",
    "                inputMonth = driver.find_element_by_id(\"cphBody_cboMonth\")\n",
    "                inputMonth.send_keys(str(months[j]))\n",
    "\n",
    "                #wait stage for state dropdown to appear\n",
    "                wait = WebDriverWait(driver, 180)\n",
    "                wait.until(EC.presence_of_element_located((By.ID, 'cphBody_cboState')))\n",
    "\n",
    "                #finds dropdown for state and inputs valid state\n",
    "                inputState = driver.find_element_by_id(\"cphBody_cboState\")\n",
    "                if(checkElementInDropdown('cphBody_cboState', str(states[k]))):\n",
    "                    inputState.send_keys(str(states[k]))\n",
    "                    \n",
    "                    #wait stage for commodity dropdown to appear\n",
    "                    wait.until(EC.presence_of_element_located((By.ID, 'cphBody_cboCommodity')))\n",
    "                    \n",
    "                    #finds dropdown for commodity and inputs valid commodity\n",
    "                    inputCrop = driver.find_element_by_id('cphBody_cboCommodity')\n",
    "                    if(checkElementInDropdown('cphBody_cboCommodity',str(crops[l]))):\n",
    "                        inputCrop.send_keys(str(crops[l]))\n",
    "                        \n",
    "                        #wait stage for submit button\n",
    "                        wait.until(EC.presence_of_element_located((By.ID, 'cphBody_btnSubmit')))\n",
    "                        \n",
    "                        #clicks submit button\n",
    "                        driver.find_element_by_name(\"ctl00$cphBody$btnSubmit\").click()\n",
    "                        \n",
    "                        #wait stage for page to finish loading, not necessary but there as a precaution\n",
    "                        time.sleep(3)\n",
    "                        \n",
    "                        #checks to see if table is present on page refresh\n",
    "                        if(checkElementExistsByID('cphBody_gridRecords')):\n",
    "                            tab_data = driver.find_element_by_id(\"cphBody_gridRecords\")\n",
    "                            \n",
    "                            #scrapes the rows from teh table\n",
    "                            list_rows = [[cell.text for cell in row.find_elements_by_tag_name('td')]\n",
    "                                         for row in tab_data.find_elements_by_tag_name('tr')]\n",
    "                            \n",
    "                            #puts rows into a dataframe\n",
    "                            df = pd.DataFrame(list_rows)\n",
    "                            \n",
    "                            #sets the column names as they appear in the table\n",
    "                            df.columns=['Market', 'Arrival Date','Arrivals (Tonnes)','Variety','Minimum Price(Rs./Quintal)',\n",
    "                                        'Maximum Price(Rs./Quintal)','Modal Price(Rs./Quintal)']\n",
    "                            df = df.iloc[1:]\n",
    "                            \n",
    "                            #replaces white space in table with nan value\n",
    "                            df = df.apply(lambda x: x.str.strip()).replace('', np.nan)\n",
    "\n",
    "                            #nan value is then converted into the appropriate market name\n",
    "                            for val in range(len(df['Market'])):\n",
    "                                if(pd.isnull(df['Market'].values[val])):\n",
    "                                    #Sets nan value to current market\n",
    "                                    df['Market'].values[val] = market\n",
    "                                else:\n",
    "                                    #Sets the market to the current value, indicates a new market\n",
    "                                    market=df['Market'].values[val]\n",
    "\n",
    "                            #scraped and cleaned data is saved to csv file in directory created above \n",
    "                            df.to_csv(years[i]+' '+crops[l]+\"/\"+saveFile,index=False)\n",
    "                            logs.append('File '+saveFile+' scraped and saved')\n",
    "                            print('File '+saveFile+' scraped and saved')\n",
    "                            \n",
    "                            #appends queried page information to scraped pages list\n",
    "                            scrapedPages.append(years[i]+' '+months[j]+' '+states[k]+' '+crops[l])\n",
    "\n",
    "                            #navigates the browser back to the form page to continue loop\n",
    "                            driver.back()\n",
    "                            \n",
    "                        else:\n",
    "                            #no table is present (no data was reported)\n",
    "                            logs.append('No table for: '+saveFile+'. No file saved')\n",
    "                            print('No table for: '+saveFile+'. No file saved')\n",
    "                            \n",
    "                            #appends queried page information to unscraped pages list\n",
    "                            unscrapedPages.append(years[i]+' '+months[j]+' '+states[k]+' '+crops[l])\n",
    "                            driver.back()\n",
    "                        \n",
    "                    else:\n",
    "                        #the queried crop is not in the dropdown tab\n",
    "                        logs.append('No crop for: '+saveFile+'. No file saved')\n",
    "                        print('No crop for: '+saveFile+'. No file saved')\n",
    "                        \n",
    "                        #appends queried page information to unscraped pages list\n",
    "                        unscrapedPages.append(years[i]+' '+months[j]+' '+states[k]+' '+crops[l])\n",
    "                        \n",
    "                        #refreshes the page to continue the loop\n",
    "                        driver.refresh()\n",
    "                else:\n",
    "                    #the queried state is not in the dropdown tab\n",
    "                    logs.append('No state for: '+saveFile+'. No file saved')\n",
    "                    print('No state for: '+saveFile+'. No file saved')\n",
    "                    \n",
    "                    #appends queried page information to unscraped pages list\n",
    "                    unscrapedPages.append(years[i]+' '+months[j]+' '+states[k]+' '+crops[l])\n",
    "                    \n",
    "                    #refreshes the page to continue the loop\n",
    "                    driver.refresh()\n",
    "                    \n",
    "            #end duration for month\n",
    "            end = time.time()\n",
    "            #set final time stamp\n",
    "            endTS = datetime.datetime.now()\n",
    "            #difference between the start and end times calculated to be a positive float\n",
    "            diff = (start-end)*-1\n",
    "\n",
    "            #sets the interval for output\n",
    "            interval = 'seconds'\n",
    "            if(diff > 60):\n",
    "                diff = diff/60\n",
    "                interval = 'minutes'\n",
    "            else:\n",
    "                interval = 'seconds'\n",
    "\n",
    "            #appends the difference to runTimes list\n",
    "            runTimes.append(diff)\n",
    "            #sets duration to the average of values in runTimes list. updates with every iteration to increase accuracy\n",
    "            duration = avg(runTimes)\n",
    "\n",
    "            #output statement for current month iteration for all states\n",
    "            logs.append('It took '+str(diff)+' '+interval+' to execute '+months[j]+' '+years[i]+\" for all states. Start: \"+str(startTS)[11:19]+' End: '+str(endTS)[11:19])\n",
    "            print('It took '+str(diff)+' '+interval+' to execute '+months[j]+' '+years[i]+\" for all states. Start: \"+str(startTS)[11:19]+' End: '+str(endTS)[11:19])\n",
    "            #adds time to the end time stamp to predict when next month will finish executing \n",
    "            if(interval=='minutes'):\n",
    "                logs.append('Around '+str(endTS+timedelta(minutes=duration))[11:19]+' is when the next month is expected to finish')\n",
    "                print('Around '+str(endTS+timedelta(minutes=duration))[11:19]+' is when the next month is expected to finish')\n",
    "            else:\n",
    "                logs.append('Around '+str(endTS+timedelta(seconds=duration))[11:19]+' is when the next month is expected to finish')\n",
    "                logs.append('\\n')\n",
    "                print('Around '+str(endTS+timedelta(seconds=duration))[11:19]+' is when the next month is expected to finish')\n",
    "            print()\n",
    "\n",
    "            #sets the start duration to the previous end duration\n",
    "            start = end\n",
    "            #sets the initial time stamp to the final timestamp\n",
    "            startTS = endTS\n",
    "        \n",
    "            #calls method that writes logs to text file\n",
    "            log(logs,logFileSave[i])\n",
    "            #reset logs so data doesnt accumulate as the years increase\n",
    "            logs = []\n",
    "            print('Logs saved')\n",
    "        \n",
    "#creates a log file and saves the scraped pages list                   \n",
    "with open(\"Scraped Page Logs.txt\", \"w\") as output:\n",
    "    output.write(str(scrapedPages))\n",
    "    \n",
    "#creates a log file and saves the unscraped pages list      \n",
    "with open(\"Unscraped Page Logs.txt\", \"w\") as output:\n",
    "    output.write(str(unscrapedPages))\n",
    "    \n",
    "#indicates that the program has completed\n",
    "driver.close()\n",
    "print('Done')\n",
    "\n",
    "totalPages = len(scrapedPages)+len(unscrapedPages)\n",
    "print(str(len(scrapedPages))+' out of '+str(totalPages)+' were able to be scraped')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
