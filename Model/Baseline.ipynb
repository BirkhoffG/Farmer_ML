{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Farmer Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3ab1tUgT5m18",
        "Lmo0HDTr7bwc",
        "zVngsxxH7skU",
        "bVxVVOcn72lC",
        "zy62mAMyAjBM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVe3jMTR57J-",
        "colab_type": "text"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9c-e8228Nh-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "fa8fd424-d9a6-4d96-988a-0d02a03911bd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDMYeDOK-gUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"/content/drive/My Drive/data/arrays\" ./arrays"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4-a3jOh43JW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ae74d9e-2c22-4bf0-c832-e045d40bfb76"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.dataset import TensorDataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import logging\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgqYOP955hpb",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc-lhAP5G2dN",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_OZJXn-G2Ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, inputDim, hiddenNum, outputDim, seq_len=90, output_len=1, layerNum=3, drop_out=0.):\n",
        "\n",
        "        super(LSTM, self).__init__()\n",
        "        # hidden cell numbers\n",
        "        self.hiddenNum = hiddenNum\n",
        "        # input dimension\n",
        "        self.inputDim = inputDim\n",
        "        # output dimension\n",
        "        self.outputDim = outputDim\n",
        "        # layer number\n",
        "        self.layerNum = layerNum\n",
        "        # sequence length\n",
        "        self.seq_len = seq_len\n",
        "        # output length\n",
        "        self.output_len = output_len\n",
        "\n",
        "        # LSTM cell\n",
        "        self.lstm = nn.LSTM(input_size=self.inputDim, hidden_size=self.hiddenNum,\n",
        "                            num_layers=self.layerNum, dropout=drop_out,\n",
        "                            batch_first=True)\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.layer_norm = nn.LayerNorm(hiddenNum)\n",
        "\n",
        "        self.fc = nn.Linear(self.hiddenNum, self.outputDim)\n",
        "        self.final_fc = nn.Linear(self.seq_len, self.output_len)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch, input_len, output_len]\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.layerNum * 1, batch_size, self.hiddenNum)\n",
        "        c0 = torch.zeros(self.layerNum * 1, batch_size, self.hiddenNum)\n",
        "        h0, c0 = h0.to(device), c0.to(device)\n",
        "\n",
        "        # output = [batch_size, seq_len, hidden_num]\n",
        "        # hn = ([num_layer, batch_len, hidden_num],\n",
        "        #       [num_layer, batch_len, hidden_num])\n",
        "        output, hn = self.lstm(x, (h0, c0))\n",
        "        output = self.layer_norm(output)\n",
        "\n",
        "        #         output = output.view(output.size(0)*output.size(1), output.size(2))\n",
        "        fc_output = self.fc(output)\n",
        "        fc_output = self.dropout(fc_output)\n",
        "        fc_output = fc_output.squeeze()\n",
        "        fc_output = self.final_fc(fc_output)\n",
        "        \n",
        "        # fc_output = self.fc(output[:, -1, :])\n",
        "\n",
        "        return fc_output\n",
        "\n",
        "    def weight_init(self):\n",
        "        for param in self.lstm.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            else:\n",
        "                nn.init.normal_(param.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ab1tUgT5m18",
        "colab_type": "text"
      },
      "source": [
        "### Attention-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5F2npgT5gw4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AttenLSTM(nn.Module):\n",
        "    def __init__(self, inputDim, hiddenNum, outputDim, seq_len=90, output_len=1, layerNum=3, drop_out=0.):\n",
        "\n",
        "        super(AttenLSTM, self).__init__()\n",
        "        # hidden cell numbers\n",
        "        self.hiddenNum = hiddenNum\n",
        "        # input dimension\n",
        "        self.inputDim = inputDim\n",
        "        # output dimension\n",
        "        self.outputDim = outputDim\n",
        "        # layer number\n",
        "        self.layerNum = layerNum\n",
        "        # sequence length\n",
        "        self.seq_len = seq_len\n",
        "        # output length\n",
        "        self.output_len = output_len\n",
        "\n",
        "        # LSTM cell\n",
        "        self.lstm = nn.LSTM(input_size=self.inputDim, hidden_size=self.hiddenNum,\n",
        "                            num_layers=self.layerNum, batch_first=True)\n",
        "        self.atten = ScaledDotProductAttention(dropout=drop_out)\n",
        "\n",
        "        self.lstm_decoder = nn.LSTM(input_size=self.hiddenNum, hidden_size=self.hiddenNum,\n",
        "                                    num_layers=self.layerNum, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        self.fc = nn.Linear(self.hiddenNum, self.outputDim)\n",
        "        self.final_fc = nn.Linear(self.seq_len, self.output_len)\n",
        "        self.weight_init()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = [batch, input_len, output_len]\n",
        "        batch_size = x.size(0)\n",
        "        h0 = torch.zeros(self.layerNum * 1, batch_size, self.hiddenNum)\n",
        "        c0 = torch.zeros(self.layerNum * 1, batch_size, self.hiddenNum)\n",
        "        h0, c0 = h0.to(device), c0.to(device)\n",
        "\n",
        "        # output = [batch_size, seq_len, hidden_num]\n",
        "        # hn = ([num_layer, batch_len, hidden_num],\n",
        "        #       [num_layer, batch_len, hidden_num])\n",
        "        output, hn = self.lstm(x, (h0, c0))\n",
        "\n",
        "        atten_output, _ = self.atten(output, output, output)\n",
        "        # print(f\"atten_output size: {atten_output.size()}\")\n",
        "\n",
        "        decoder_output, hn_d = self.lstm_decoder(atten_output, hn)\n",
        "\n",
        "        # fc_output = F.relu(self.fc(atten_output))\n",
        "        fc_output = self.fc(decoder_output)\n",
        "        fc_output = fc_output.squeeze()\n",
        "        fc_output = self.final_fc(fc_output)\n",
        "\n",
        "        return fc_output\n",
        "\n",
        "    def weight_init(self):\n",
        "        for param in self.lstm.parameters():\n",
        "            if len(param.shape) >= 2:\n",
        "                nn.init.orthogonal_(param.data)\n",
        "            else:\n",
        "                nn.init.normal_(param.data)\n",
        "\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "    def forward(self, q, k, v, scale=None):\n",
        "        attention = torch.bmm(q, k.transpose(1, 2))\n",
        "        if scale:\n",
        "            attention = attention * scale\n",
        "        # calculate softmax\n",
        "        attention = self.softmax(attention)\n",
        "        # add dropout\n",
        "        attention = self.dropout(attention)\n",
        "        # context\n",
        "        context = torch.bmm(attention, v)\n",
        "        return context, attention\n",
        "\n",
        "class PositionalWiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, model_dim=512, ffn_dim=2048, dropout=0.0):\n",
        "        super(PositionalWiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Conv1d(model_dim, ffn_dim, 1)\n",
        "        self.w2 = nn.Conv1d(ffn_dim, model_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(model_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = x.transpose(1, 2)\n",
        "        output = self.w2(F.tanh(self.w1(output)))\n",
        "        output = self.dropout(output.transpose(1, 2))\n",
        "\n",
        "        # add residual and norm layer\n",
        "        output = self.layer_norm(x + output)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H40qYdm7c9S",
        "colab_type": "text"
      },
      "source": [
        "### TCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qke4gFq5u4n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.nn.utils import weight_norm\n",
        "\n",
        "class TCN(nn.Module):\n",
        "    def __init__(self, input_size, output_size, num_channels, input_len, output_len,\n",
        "                 kernel_size=3, dropout=0.3, feature=None):\n",
        "        super(TCN, self).__init__()\n",
        "\n",
        "        # TCN nets\n",
        "        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n",
        "\n",
        "        # set features\n",
        "        self.feature = feature\n",
        "\n",
        "        self.decoder = nn.Linear(num_channels[-1], output_size)\n",
        "        self.final_fc = nn.Linear(input_len, output_len)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # self.encoder.weight.data.normal_(0, 0.01)\n",
        "        #         nn.init.xavier_normal_(self.tcn)\n",
        "        nn.init.xavier_normal_(self.final_fc.weight.data)\n",
        "        nn.init.normal_(self.final_fc.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\n",
        "        # emb = self.drop(input)\n",
        "        if self.feature == 'pri':\n",
        "            x = x[:, :, :1]\n",
        "        elif self.feature == 'vol':\n",
        "            x = x[:, :, -1].unsqueeze(2)\n",
        "        y = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
        "        # print(f\"price_y shape: {price_y.size()}; volume_y shape: {volume_y.size()}\")\n",
        "        # print(f\"y shape: {y.size()}\")\n",
        "        # y = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n",
        "        y = self.decoder(y)\n",
        "        y = self.final_fc(y[:, :, 0])\n",
        "        # print(\"y.size: \", y.size())\n",
        "        return self.dropout(y)\n",
        "\n",
        "\n",
        "class Chomp1d(nn.Module):\n",
        "    def __init__(self, chomp_size):\n",
        "        super(Chomp1d, self).__init__()\n",
        "        self.chomp_size = chomp_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x[:, :, :-self.chomp_size].contiguous()\n",
        "\n",
        "\n",
        "class TemporalBlock(nn.Module):\n",
        "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
        "        super(TemporalBlock, self).__init__()\n",
        "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp1 = Chomp1d(padding)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
        "                                           stride=stride, padding=padding, dilation=dilation))\n",
        "        self.chomp2 = Chomp1d(padding)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
        "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
        "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
        "        self.relu = nn.ReLU()\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        # init conv1\n",
        "        nn.init.xavier_uniform_(self.conv1.weight, gain=np.sqrt(2))\n",
        "        # nn.init.normal_(self.conv1.weight.data)\n",
        "        if self.conv1.bias is not None:\n",
        "            nn.init.normal_(self.conv1.bias.data)\n",
        "        # init conv2\n",
        "        nn.init.xavier_uniform_(self.conv2.weight, gain=np.sqrt(2))\n",
        "        # nn.init.normal_(self.conv2.weight.data)\n",
        "        if self.conv2.bias is not None:\n",
        "            nn.init.normal_(self.conv2.bias.data)\n",
        "        if self.downsample is not None:\n",
        "            nn.init.xavier_uniform_(self.downsample.weight, gain=np.sqrt(2))\n",
        "            # nn.init.normal_(self.downsample.weight.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.net(x)\n",
        "        res = x if self.downsample is None else self.downsample(x)\n",
        "        return self.relu(out + res)\n",
        "\n",
        "\n",
        "class TemporalConvNet(nn.Module):\n",
        "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
        "        super(TemporalConvNet, self).__init__()\n",
        "        layers = []\n",
        "        num_levels = len(num_channels)\n",
        "        for i in range(num_levels):\n",
        "            dilation_size = 2 ** i\n",
        "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
        "            out_channels = num_channels[i]\n",
        "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
        "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
        "\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lmo0HDTr7bwc",
        "colab_type": "text"
      },
      "source": [
        "### Standard Wide & Deep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiKrEVXF7rE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVngsxxH7skU",
        "colab_type": "text"
      },
      "source": [
        "### PECAD - Single TCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNUemuV7vjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVxVVOcn72lC",
        "colab_type": "text"
      },
      "source": [
        "### PECAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRoxURvD73_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy62mAMyAjBM",
        "colab_type": "text"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2kko1qtCuhc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ArrayDataset(TensorDataset):\n",
        "\n",
        "    def __init__(self, *arrs):\n",
        "        super(ArrayDataset, self).__init__()\n",
        "        # init tensors\n",
        "        self.tensors = [torch.from_numpy(arr) for arr in arrs]\n",
        "        assert all(self.tensors[0].size(0) == tensor.size(0) for tensor in self.tensors)\n",
        "\n",
        "    def data_loader(self, batch_size=128, shuffle=True, num_workers=4):\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "It9joQYl3ezs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "def RMSE(output: torch.tensor, target: torch.tensor, std: float, mean: float):\n",
        "    \"\"\"\n",
        "    $$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}(\\hat{y}_t^2-y_t^2)}$$\n",
        "\n",
        "    :param output: model predicted tensor\n",
        "    :param target: target tensor\n",
        "    :param std: standard deviation\n",
        "    :param mean: mean\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    normalized_output = output * std + mean\n",
        "    normalized_target = target * std + mean\n",
        "\n",
        "    return torch.sqrt(torch.mean((normalized_output - normalized_target) ** 2))\n",
        "\n",
        "def val_RMSE(pred_arr, tar_arr, std, mean):\n",
        "    nomalized_pre_arr = pred_arr * std + mean\n",
        "    nomalized_test_y = tar_arr * std + mean\n",
        "    #     print(f\"predict array shape: {nomalized_pre_arr.shape}; target array shape: {nomalized_test_y.shape}\")\n",
        "    return np.sqrt(mse(nomalized_pre_arr, nomalized_test_y))\n",
        "\n",
        "\n",
        "def list2arr(li: list):\n",
        "    \"\"\"\n",
        "    convert list to numpy array\n",
        "    :param li: list\n",
        "    :return: converted np array\n",
        "    \"\"\"\n",
        "    arr = np.array(li)\n",
        "    return arr.reshape(arr.shape[0], arr.shape[1])\n",
        "\n",
        "\n",
        "def validate(model: nn.Module, val_loader: DataLoader, std, mean):\n",
        "    \"\"\"\n",
        "    validate the test set in the model\n",
        "    :param model: neural network modal\n",
        "    :param val_loader: test loader\n",
        "    :param std: standard deviation\n",
        "    :param mean: mean value\n",
        "    :return: RMSE on validation set; loss list on validation set\n",
        "    \"\"\"\n",
        "    # track values\n",
        "    pred_list, tar_list, loss_list = tuple([] for _ in range(3))\n",
        "    # L2 loss\n",
        "    criterion = nn.MSELoss()\n",
        "    # model evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(val_loader):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            result = model(x.float())\n",
        "\n",
        "            result_list = result.cpu().data.numpy().tolist()\n",
        "            pred_list.extend(result_list)\n",
        "            tar_list.extend(y.cpu().data.numpy().tolist())\n",
        "\n",
        "            loss = criterion(result, y.float()).cpu().data.numpy()\n",
        "            loss_list.append(loss)\n",
        "\n",
        "    pred_arr = list2arr(pred_list)\n",
        "    tar_arr = list2arr(tar_list)\n",
        "    assert pred_arr.shape == tar_arr.shape\n",
        "\n",
        "    rmse = val_RMSE(pred_arr, tar_arr, std, mean)\n",
        "    return rmse, loss_list\n",
        "\n",
        "\n",
        "def poly_lr_scheduler(optimizer, init_lr, iter, lr_decay_iter=1,\n",
        "                      max_iter=50, power=0.9):\n",
        "    \"\"\"\n",
        "    Polynomial decay of learning rate\n",
        "    :param init_lr is base learning rate\n",
        "    :param iter is a current iteration\n",
        "    :param lr_decay_iter how frequently decay occurs, default is 1\n",
        "    :param max_iter is number of maximum iterations\n",
        "    :param power is a polymomial power\n",
        "\n",
        "    \"\"\"\n",
        "    if iter % lr_decay_iter or iter > max_iter:\n",
        "        return optimizer\n",
        "\n",
        "    lr = init_lr*(1 - iter/max_iter)**power\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    return lr\n",
        "\n",
        "\n",
        "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
        "          epochs: int, lr: float, device: torch.device,\n",
        "          train_std: float, train_mean: float, \n",
        "          val_std: float, val_mean: float, criterion=None):\n",
        "    \"\"\"\n",
        "    Train model on train_loader and track the validation value on val_loader.\n",
        "\n",
        "    :param model: training neural network model\n",
        "    :param train_loader: train set loader\n",
        "    :param val_loader: validation set loader\n",
        "    :param epochs: training epoches\n",
        "    :param lr: learning rate\n",
        "    :param train_std: training set's standard deviation\n",
        "    :param train_mean: training set's average value\n",
        "    :param val_std: validation set's standard deviation\n",
        "    :param val_mean: validation set's average value\n",
        "    :param log: logger object\n",
        "    :param criterion: loss function (default: L1 loss)\n",
        "    :return: loss_list, rmse_list, val_loss_list, val_rmse_list\n",
        "    \"\"\"\n",
        "    # loss function\n",
        "    criterion = nn.L1Loss() if criterion is None else criterion\n",
        "    # optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-9)\n",
        "    # track loss list\n",
        "    loss_list, rmse_list, val_loss_list, val_rmse_list = tuple([] for _ in range(4))\n",
        "    total_steps = len(train_loader)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # start training\n",
        "        model.train()\n",
        "        poly_lr_scheduler(optimizer, lr, epoch+1)\n",
        "\n",
        "        for ix, (x, y) in enumerate(train_loader):\n",
        "            # track starting time\n",
        "            start_time = time.time()\n",
        "\n",
        "            # clear the accumulated gradient before each instance\n",
        "            model.zero_grad()\n",
        "\n",
        "            # prepare the data and label\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # run the forward pass\n",
        "            outputs = model(x.float())\n",
        "\n",
        "            # Compute the loss, gradients, and update the parameters\n",
        "            loss = criterion(outputs, y.float())\n",
        "\n",
        "            # back propogation\n",
        "            loss.backward()\n",
        "            # update parameters\n",
        "            optimizer.step()\n",
        "            # clear gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # append loss to the loss list\n",
        "            rmse = RMSE(outputs, y.float(), train_std, train_mean)\n",
        "            rmse_list.append(rmse)\n",
        "            loss_list.append(loss)\n",
        "\n",
        "            # print in every 50 episodes\n",
        "            if (ix+1) % 50 == 0:\n",
        "                print(f'Epoch [{epoch + 1}/{epochs}], Step [{ix + 1}/{total_steps}], '\n",
        "                      f'Time [{time.time() - start_time:.6f} sec], Avg loss: {sum(loss_list[-50:])/50: .4f}, '\n",
        "                      f'Avg RMSE: {sum(rmse_list[-50:])/50: .4f}')\n",
        "\n",
        "        # validation\n",
        "        model.eval()\n",
        "        print(\"Validating on the testing set...\")\n",
        "        # TODO: validate crops separately\n",
        "        val_rmse, val_loss_list = validate(model, val_loader, val_std, val_mean)\n",
        "        val_avg_loss = np.mean(val_loss_list)\n",
        "        val_rmse_list.append(val_rmse)\n",
        "        val_loss_list.append(val_avg_loss)\n",
        "        print(f\"RMSE: {val_rmse: .4f}; avg loss: {val_avg_loss: .4f}\")\n",
        "\n",
        "    return loss_list, rmse_list, val_loss_list, val_rmse_list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2UDHET1AmAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def norm_arrays(*arrays, std, mean):\n",
        "    print(f\"std: {std}, mean: {mean}\")\n",
        "    return tuple((arr - mean) / std for arr in arrays)\n",
        "\n",
        "\n",
        "def save_arrays(**arrs):\n",
        "    for name, li in arrs.items():\n",
        "        log.info(f\"Saving {data_folder}/{name}.npy\")\n",
        "        np.save(f'{data_folder}/{name}.npy', np.array(li))\n",
        "\n",
        "\n",
        "def load_numpys(path, files: list):\n",
        "    return tuple(np.load(f\"{path}/{file}.npy\") for file in files)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVf6AQao79TQ",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXMGl9F2KmdL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "1b13b64c-75a0-44de-c0d0-2f7eeeeb2971"
      },
      "source": [
        "\n",
        "DAYS = [\"4 Days\", \"6 Days\", \"9 Days\"]\n",
        "MODELS = [\"RF\", \"GTB\", \"Attention-LSTM\", \"TCN\", \"Standard Wide & Deep\", \n",
        "          \"PECAD - Single TCN\", \"PECAD\"]\n",
        "CROPS = ['Brinjal', 'Green Chilli', 'Tomato']\n",
        "LENGTHS = [90, 60, 40]\n",
        "\n",
        "\n",
        "result = {\n",
        "    model: {\n",
        "        day: { crop: None for crop in CROPS } for day in DAYS \n",
        "    } for model in MODELS \n",
        "}\n",
        "import pprint as p\n",
        "p.pprint(result)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Attention-LSTM': {'4 Days': {'Brinjal': None,\n",
            "                               'Green Chilli': None,\n",
            "                               'Tomato': None},\n",
            "                    '6 Days': {'Brinjal': None,\n",
            "                               'Green Chilli': None,\n",
            "                               'Tomato': None},\n",
            "                    '9 Days': {'Brinjal': None,\n",
            "                               'Green Chilli': None,\n",
            "                               'Tomato': None}},\n",
            " 'GTB': {'4 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "         '6 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "         '9 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None}},\n",
            " 'PECAD': {'4 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "           '6 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "           '9 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None}},\n",
            " 'PECAD - Single TCN': {'4 Days': {'Brinjal': None,\n",
            "                                   'Green Chilli': None,\n",
            "                                   'Tomato': None},\n",
            "                        '6 Days': {'Brinjal': None,\n",
            "                                   'Green Chilli': None,\n",
            "                                   'Tomato': None},\n",
            "                        '9 Days': {'Brinjal': None,\n",
            "                                   'Green Chilli': None,\n",
            "                                   'Tomato': None}},\n",
            " 'RF': {'4 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "        '6 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "        '9 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None}},\n",
            " 'Standard Wide & Deep': {'4 Days': {'Brinjal': None,\n",
            "                                     'Green Chilli': None,\n",
            "                                     'Tomato': None},\n",
            "                          '6 Days': {'Brinjal': None,\n",
            "                                     'Green Chilli': None,\n",
            "                                     'Tomato': None},\n",
            "                          '9 Days': {'Brinjal': None,\n",
            "                                     'Green Chilli': None,\n",
            "                                     'Tomato': None}},\n",
            " 'TCN': {'4 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "         '6 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None},\n",
            "         '9 Days': {'Brinjal': None, 'Green Chilli': None, 'Tomato': None}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Dw2M3QKf3H",
        "colab_type": "text"
      },
      "source": [
        "### DL Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYWlLKlh7-2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 512\n",
        "\n",
        "for crop in CROPS:\n",
        "    for length in LENGTHS:\n",
        "        day = f\"{int(360/length)} Days\"\n",
        "        path = f\"./arrays/{crop}/train_{length}_01_test_{length}_01\"\n",
        "        pri_train_x, pri_train_y, vol_train_x, vol_train_y, \\\n",
        "            pri_test_x, pri_test_y, vol_test_x, vol_test_y = \\\n",
        "                load_numpys(path, [\"pri_train_x\", \"pri_train_y\", \"vol_train_x\", \"vol_train_y\", \n",
        "                                \"pri_test_x\", \"pri_test_y\", \"vol_test_x\", \"vol_test_y\"])\n",
        "\n",
        "        # normalization\n",
        "        pri_std, pri_mean = np.std(pri_train_x), np.mean(pri_train_x)\n",
        "        vol_std, vol_mean = np.std(vol_train_x), np.mean(vol_train_x)\n",
        "        pri_train_x, pri_train_y, pri_test_x, pri_test_y = \\\n",
        "            norm_arrays(pri_train_x, pri_train_y, pri_test_x, pri_test_y, \n",
        "                        std=pri_std, mean=pri_mean)\n",
        "        vol_train_x, vol_train_y, vol_test_x, vol_test_y = \\\n",
        "            norm_arrays(vol_train_x, vol_train_y, vol_test_x, vol_test_y,\n",
        "                        std=vol_std, mean=vol_mean)\n",
        "        print(f\"Price std: {pri_std}, Price Mean: {pri_mean}\")\n",
        "        print(f\"Volume std: {vol_std}, Volume Mean: {vol_mean}\")\n",
        "        print(f\"pri_train_x: {pri_train_x}\")\n",
        "        print(f\"price_train_y: {pri_train_y}\")\n",
        "        \n",
        "        # train_x/train_y\n",
        "        train_x = np.stack((pri_train_x, vol_train_x), axis=-1)\n",
        "        train_y = np.copy(pri_train_y)\n",
        "        print(f\"train_x shape: {train_x.shape}, train_y shape: {train_y.shape}\")\n",
        "\n",
        "        # test_x/test_y\n",
        "        test_x = np.stack((pri_test_x, vol_test_x), axis=-1)\n",
        "        test_y = np.copy(pri_test_y)\n",
        "\n",
        "        # train/test loader\n",
        "        train_loader = ArrayDataset(train_x, train_y).data_loader(batch_size=batch_size)\n",
        "        test_loader = ArrayDataset(test_x, test_y).data_loader(batch_size=batch_size)\n",
        "\n",
        "        # Attention-LSTM\n",
        "        # lstm = LSTM(inputDim=2, hiddenNum=16, outputDim=1, seq_len=length, layerNum=1)\n",
        "        lstm = AttenLSTM(inputDim=2, hiddenNum=32, outputDim=1, seq_len=length, layerNum=3)\n",
        "        lstm.to(device)\n",
        "\n",
        "        _, _, _, val_rmse_list = \\\n",
        "            train(model=lstm, train_loader=train_loader, val_loader=test_loader, \n",
        "                epochs=2, lr=0.01, train_std=pri_std, train_mean=pri_mean,\n",
        "                val_std=pri_std, val_mean=pri_mean, device=device)\n",
        "        result[\"Attention-LSTM\"][day][crop] = min(val_rmse_list)\n",
        "\n",
        "        # TCN\n",
        "        tcn = TCN(input_size=2, output_size=1, num_channels=[16, 8, 2], \n",
        "                  input_len=90, output_len=1, kernel_size=4)\n",
        "        tcn.to(device)\n",
        "        _, _, _, val_rmse_list = \\\n",
        "            train(model=tcn, train_loader=train_loader, val_loader=test_loader, \n",
        "                epochs=20, lr=0.01, train_std=pri_std, train_mean=pri_mean,\n",
        "                val_std=pri_std, val_mean=pri_mean, device=device)\n",
        "        result[\"TCN\"][day][crop] = min(val_rmse_list)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_fdf2Q3QrP4",
        "colab_type": "text"
      },
      "source": [
        "### ML Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlEtEIwzQvfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "outputId": "ff6d7f97-7ca2-4b1d-8b96-70419ce4436d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor as RF, \\\n",
        "        GradientBoostingRegressor as GBT\n",
        "\n",
        "from sklearn.metrics import mean_squared_error as mse\n",
        "\n",
        "for crop in CROPS:\n",
        "    for length in LENGTHS:\n",
        "        day = f\"{int(360/length)} Days\"\n",
        "        path = f\"./arrays/{crop}/train_{length}_01_test_{length}_01\"\n",
        "\n",
        "        # load data\n",
        "        pri_train_x, pri_train_y, vol_train_x, vol_train_y, \\\n",
        "            pri_test_x, pri_test_y, vol_test_x, vol_test_y = \\\n",
        "                load_numpys(path, [\"pri_train_x\", \"pri_train_y\", \"vol_train_x\", \"vol_train_y\", \n",
        "                                \"pri_test_x\", \"pri_test_y\", \"vol_test_x\", \"vol_test_y\"])\n",
        "        \n",
        "        # train/test set\n",
        "        train_x = np.concatenate((pri_train_x, vol_train_x), axis=1)\n",
        "        train_y = np.copy(pri_train_y)\n",
        "        test_x = np.concatenate((pri_test_x, vol_test_x), axis=1)\n",
        "        test_y = np.copy(pri_test_y)\n",
        "\n",
        "        # random forest\n",
        "        rf = RF(n_estimators=100, max_depth=5)\n",
        "        rf.fit(train_x, train_y)\n",
        "        rf_pred = rf.predict(test_x)\n",
        "        result[\"RF\"][day][crop] = np.sqrt(mse(test_y, rf_pred))\n",
        "\n",
        "        # Gradient Boosting Tree\n",
        "        gbt = GBT(n_estimators=100, max_depth=5)\n",
        "        gbt.fit(train_x, train_y)\n",
        "        gbt_pred = gbt.predict(test_x)\n",
        "        result[\"GBT\"][day][crop] = np.sqrt(mse(test_y, gbt_pred))\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f3bc894e28ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         pri_train_x, pri_train_y, vol_train_x, vol_train_y,             pri_test_x, pri_test_y, vol_test_x, vol_test_y =                 load_numpys(path, [\"pri_train_x\", \"pri_train_y\", \"vol_train_x\", \"vol_train_y\", \n\u001b[0m\u001b[1;32m     12\u001b[0m                                 \"pri_test_x\", \"pri_test_y\", \"vol_test_x\", \"vol_test_y\"])\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_numpys' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaOhFjdhQvCg",
        "colab_type": "text"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd0nJ-VoOiro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p.pprint(result)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}